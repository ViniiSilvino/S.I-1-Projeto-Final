"""
Otimiza√ß√£o de Hiperpar√¢metros usando Optuna
"""
import optuna
from optuna.samplers import TPESampler
import xgboost as xgb
import numpy as np
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.metrics import f1_score, make_scorer
from src.utils import logger
import json
import os

class HyperparameterTuner:
    """Classe para otimiza√ß√£o de hiperpar√¢metros com Optuna"""
    
    def __init__(self, X_train, y_train, n_trials=50, cv_folds=5):
        """
        Inicializa o tuner
        
        Args:
            X_train: Features de treino
            y_train: Target de treino
            n_trials: N√∫mero de tentativas do Optuna
            cv_folds: Folds para valida√ß√£o cruzada
        """
        self.X_train = X_train
        self.y_train = y_train
        self.n_trials = n_trials
        self.cv_folds = cv_folds
        self.best_params = None
        self.study = None
        
    def objective(self, trial):
        """
        Fun√ß√£o objetivo para o Optuna otimizar
        
        Args:
            trial: Trial do Optuna
        
        Returns:
            F1-Score macro m√©dio da valida√ß√£o cruzada
        """
        # Definir espa√ßo de busca de hiperpar√¢metros
        params = {
            'objective': 'multi:softmax',
            'num_class': 3,
            'eval_metric': 'mlogloss',
            'random_state': 42,
            'tree_method': 'hist',  # Mais r√°pido
            'verbosity': 0,
            
            # Hiperpar√¢metros a otimizar
            'max_depth': trial.suggest_int('max_depth', 2, 7),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
            'n_estimators': trial.suggest_int('n_estimators', 100, 500, step=50),
            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
            'gamma': trial.suggest_float('gamma', 0, 0.5),
            'subsample': trial.suggest_float('subsample', 0.6, 1.0),
            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
            'reg_alpha': trial.suggest_float('reg_alpha', 0, 1.0),
            'reg_lambda': trial.suggest_float('reg_lambda', 0, 2.0),
        }
        
        # Criar modelo
        model = xgb.XGBClassifier(**params)
        
        # Valida√ß√£o cruzada estratificada
        cv = StratifiedKFold(n_splits=self.cv_folds, shuffle=True, random_state=42)
        
        # Scorer customizado - F1 Macro (foco em todas as classes)
        scorer = make_scorer(f1_score, average='macro')
        
        # Avaliar
        scores = cross_val_score(
            model, 
            self.X_train, 
            self.y_train, 
            cv=cv, 
            scoring=scorer,
            n_jobs=-1
        )
        
        return scores.mean()
    
    def optimize(self, timeout=None):
        """
        Executa a otimiza√ß√£o
        
        Args:
            timeout: Tempo m√°ximo em segundos (None = sem limite)
        
        Returns:
            Melhores hiperpar√¢metros encontrados
        """
        logger.info("\n" + "="*60)
        logger.info("INICIANDO OTIMIZA√á√ÉO DE HIPERPAR√ÇMETROS")
        logger.info("="*60)
        logger.info(f"\nüìä Configura√ß√£o:")
        logger.info(f"  Trials: {self.n_trials}")
        logger.info(f"  CV Folds: {self.cv_folds}")
        logger.info(f"  Timeout: {timeout if timeout else 'Sem limite'}")
        logger.info(f"  M√©trica: F1-Score Macro")
        logger.info(f"\nüîç Espa√ßo de busca:")
        logger.info(f"  max_depth: [4, 10]")
        logger.info(f"  learning_rate: [0.01, 0.3]")
        logger.info(f"  n_estimators: [100, 500]")
        logger.info(f"  min_child_weight: [1, 10]")
        logger.info(f"  gamma: [0, 0.5]")
        logger.info(f"  subsample: [0.6, 1.0]")
        logger.info(f"  colsample_bytree: [0.6, 1.0]")
        logger.info(f"  reg_alpha: [0, 1.0]")
        logger.info(f"  reg_lambda: [0, 2.0]")
        
        # Criar estudo
        sampler = TPESampler(seed=42)
        self.study = optuna.create_study(
            direction='maximize',
            sampler=sampler,
            study_name='xgboost_optimization'
        )
        
        # Callback para logging
        def logging_callback(study, trial):
            logger.info(f"\nüîÑ Trial {trial.number + 1}/{self.n_trials}")
            logger.info(f"  F1-Score: {trial.value:.4f}")
            logger.info(f"  Melhor at√© agora: {study.best_value:.4f}")
        
        # Otimizar
        logger.info(f"\n‚öôÔ∏è  Iniciando otimiza√ß√£o...\n")
        
        self.study.optimize(
            self.objective,
            n_trials=self.n_trials,
            timeout=timeout,
            callbacks=[logging_callback],
            show_progress_bar=True
        )
        
        # Resultados
        self.best_params = self.study.best_params
        
        logger.info("\n" + "="*60)
        logger.info("OTIMIZA√á√ÉO CONCLU√çDA")
        logger.info("="*60)
        logger.info(f"\n‚úÖ Melhor F1-Score: {self.study.best_value:.4f}")
        logger.info(f"\nüéØ Melhores Hiperpar√¢metros:")
        for param, value in self.best_params.items():
            logger.info(f"  {param}: {value}")
        
        return self.best_params
    
    def get_best_model_params(self):
        """
        Retorna os par√¢metros completos para criar o melhor modelo
        
        Returns:
            Dict com todos os par√¢metros
        """
        if self.best_params is None:
            raise ValueError("Execute optimize() primeiro!")
        
        full_params = {
            'objective': 'multi:softmax',
            'num_class': 3,
            'eval_metric': 'mlogloss',
            'random_state': 42,
            **self.best_params
        }
        
        return full_params
    
    def save_results(self, filepath='models/optuna_results.json'):
        """
        Salva os resultados da otimiza√ß√£o
        
        Args:
            filepath: Caminho do arquivo
        """
        if self.best_params is None:
            raise ValueError("Execute optimize() primeiro!")
        
        results = {
            'best_params': self.best_params,
            'best_score': self.study.best_value,
            'n_trials': len(self.study.trials),
            'all_trials': [
                {
                    'number': trial.number,
                    'value': trial.value,
                    'params': trial.params
                }
                for trial in self.study.trials
            ]
        }
        
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        
        with open(filepath, 'w') as f:
            json.dump(results, f, indent=4)
        
        logger.info(f"\nüíæ Resultados salvos em: {filepath}")
    
    def plot_optimization_history(self, save_path='models/optimization_history.png'):
        """
        Plota o hist√≥rico de otimiza√ß√£o
        
        Args:
            save_path: Caminho para salvar o gr√°fico
        """
        try:
            import matplotlib.pyplot as plt
            
            fig = optuna.visualization.matplotlib.plot_optimization_history(self.study)
            plt.tight_layout()
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
            logger.info(f"üìä Gr√°fico de otimiza√ß√£o salvo em: {save_path}")
            plt.close()
        except ImportError:
            logger.warning("matplotlib n√£o instalado. Pulando visualiza√ß√£o.")
    
    def plot_param_importances(self, save_path='models/param_importances.png'):
        """
        Plota import√¢ncia dos hiperpar√¢metros
        
        Args:
            save_path: Caminho para salvar o gr√°fico
        """
        try:
            import matplotlib.pyplot as plt
            
            fig = optuna.visualization.matplotlib.plot_param_importances(self.study)
            plt.tight_layout()
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
            logger.info(f"üìä Import√¢ncia dos par√¢metros salva em: {save_path}")
            plt.close()
        except ImportError:
            logger.warning("matplotlib n√£o instalado. Pulando visualiza√ß√£o.")

def run_hyperparameter_optimization(X_train, y_train, n_trials=50, timeout=None):
    """
    Fun√ß√£o principal para executar otimiza√ß√£o de hiperpar√¢metros
    
    Args:
        X_train: Features de treino
        y_train: Target de treino
        n_trials: N√∫mero de trials
        timeout: Tempo m√°ximo em segundos
    
    Returns:
        Melhores par√¢metros encontrados
    """
    # Criar tuner
    tuner = HyperparameterTuner(X_train, y_train, n_trials=n_trials)
    
    # Otimizar
    best_params = tuner.optimize(timeout=timeout)
    
    # Salvar resultados
    tuner.save_results()
    
    # Plotar resultados (se matplotlib dispon√≠vel)
    tuner.plot_optimization_history()
    tuner.plot_param_importances()
    
    return best_params, tuner

def quick_tune(X_train, y_train, n_trials=20):
    """
    Otimiza√ß√£o r√°pida com menos trials
    
    Args:
        X_train: Features de treino
        y_train: Target de treino
        n_trials: N√∫mero de trials (padr√£o: 20)
    
    Returns:
        Melhores par√¢metros
    """
    logger.info("\nüöÄ MODO R√ÅPIDO - Otimiza√ß√£o com menos trials")
    return run_hyperparameter_optimization(X_train, y_train, n_trials=n_trials)

def extensive_tune(X_train, y_train, n_trials=100, timeout=3600):
    """
    Otimiza√ß√£o extensiva com mais trials e timeout
    
    Args:
        X_train: Features de treino
        y_train: Target de treino
        n_trials: N√∫mero de trials (padr√£o: 100)
        timeout: Tempo m√°ximo em segundos (padr√£o: 1 hora)
    
    Returns:
        Melhores par√¢metros
    """
    logger.info("\nüî¨ MODO EXTENSIVO - Otimiza√ß√£o completa")
    return run_hyperparameter_optimization(X_train, y_train, n_trials=n_trials, timeout=timeout)

if __name__ == "__main__":
    # Teste do m√≥dulo
    from src.etl import load_and_preprocess_data
    from src.feature_engineering import create_features
    from src.model_xgboost import SoccerPredictor
    
    logger.info("=== TESTE DE OTIMIZA√á√ÉO DE HIPERPAR√ÇMETROS ===\n")
    
    # Carregar dados
    logger.info("Carregando dados...")
    data = load_and_preprocess_data()
    master_df = create_features(data)
    
    # Preparar dados
    predictor = SoccerPredictor()
    predictor.prepare_data(master_df, use_balancing=True)
    
    # Otimiza√ß√£o r√°pida (apenas para teste)
    logger.info("\nExecutando otimiza√ß√£o r√°pida com 10 trials...")
    best_params, tuner = quick_tune(predictor.X_train, predictor.y_train, n_trials=10)
    
    logger.info("\n‚úì Teste conclu√≠do!")
    logger.info(f"Melhores par√¢metros: {best_params}")